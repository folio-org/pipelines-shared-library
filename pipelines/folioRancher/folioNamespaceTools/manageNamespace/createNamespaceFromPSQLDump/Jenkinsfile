#!groovy
import org.folio.Constants
import org.folio.models.TerraformConfig
import org.jenkinsci.plugins.workflow.libs.Library

@Library('pipelines-shared-library@RANCHER-1216') _

CONFIG_BRANCH = 'RANCHER-1216'

properties([buildDiscarder(logRotator(numToKeepStr: '20')),
            disableConcurrentBuilds(),
            parameters([folioParameters.cluster(),
                        folioParameters.namespace(),
                        folioParameters.branch(),
                        folioParameters.okapiVersion(),
                        folioParameters.configType(),
                        folioParameters.loadReference(),
                        folioParameters.loadSample(),
                        booleanParam(name: 'CONSORTIA', defaultValue: true, description: '(Optional) Set true to create consortium'),
                        booleanParam(name: 'RW_SPLIT', defaultValue: false, description: '(Optional) Set true to Enable Read/Write split'),
                        booleanParam(name: 'GREENMAIL', defaultValue: false, description: '(Optional) Set true to deploy greenmail server'),
                        booleanParam(name: 'MOCK_SERVER', defaultValue: false, description: '(Optional) Set true to deploy mock-server'),
                        booleanParam(name: 'RTR', defaultValue: false, description: '(Optional) Set true to enable RTR'),
                        string(name: 'SOURCE_DB_USERS', defaultValue: 'ecs-snapshot-users', description: '(Required) Name of users dump file'),
                        string(name: 'SOURCE_DB', defaultValue: 'ecs-snapshot', description: '(Required) Data dump file name'),
                        folioParameters.pgType(),
                        folioParameters.pgVersion(),
                        folioParameters.kafkaType(),
                        folioParameters.opensearchType(),
                        folioParameters.s3Type(),
                        string(name: 'MEMBERS', defaultValue: '', description: '(Optional) Coma separated list of GitHub teams who need an access to the namespace'),
                        folioParameters.agent(),
                        folioParameters.refreshParameters()
            ])])

ansiColor('xterm') {
  if (params.REFRESH_PARAMETERS) {
    currentBuild.result = 'ABORTED'
    error('DRY RUN BUILD, PARAMETERS REFRESHED!')
  }

  node(params.AGENT) {
    try {
      stage('Ini') {
        buildName params.CLUSTER + '-' + params.NAMESPACE + '-' + env.BUILD_ID
        buildDescription "cluster: ${params.CLUSTER}\n" + "namespace: ${params.NAMESPACE}\n" + "config: ${params.CONFIG_TYPE}"
      }
      stage('Checkout') {
        checkout scm
      }

      TerraformConfig tfConfig = new TerraformConfig('terraform/rancher/project')
        .withWorkspace("${params.CLUSTER}-${params.NAMESPACE}")

      tfConfig.addVar('pg_vol_size', 100)
      tfConfig.addVar('pg_dbname', 'folio')
      tfConfig.addVar('pg_version', params.DB_VERSION)
      tfConfig.addVar('pg_password', Constants.PG_ROOT_DEFAULT_PASSWORD)
      tfConfig.addVar('pgadmin_password', Constants.PGADMIN_DEFAULT_PASSWORD)
      tfConfig.addVar('pg_embedded', params.POSTGRESQL == 'built-in')
      tfConfig.addVar('rancher_cluster_name', params.CLUSTER)
      tfConfig.addVar('rancher_project_name', params.NAMESPACE)
      tfConfig.addVar('tenant_id', 'cs00000int')
      tfConfig.addVar('kafka_shared', params.KAFKA == 'built-in')
      tfConfig.addVar('opensearch_shared', params.OPENSEARCH != 'built-in')
      tfConfig.addVar('s3_embedded', params.S3_BUCKET != 'built-in')
      tfConfig.addVar('pgadmin4', 'true')
      tfConfig.addVar('enable_rw_split', params.RW_SPLIT)
      tfConfig.addVar('pg_ldp_user_password', Constants.PG_LDP_DEFAULT_PASSWORD)
      tfConfig.addVar('github_team_ids', folioTools.getGitHubTeamsIds("${Constants.ENVS_MEMBERS_LIST[params.NAMESPACE]},${params.MEMBERS}").collect { "\"${it}\"" })

      stage('[Destroy existing env...]') {
        folioHelm.withKubeConfig(params.CLUSTER) {
          if ((kubectl.checkNamespaceExistence('ecs-snapshot')) == 'ecs-snapshot') {
            println("ECS-SNAPSHOT ENV EXISTS!\nPROCEEDING WITH DELETE OPERATION...")
            try {
              build job: '/folioRancher/folioNamespaceTools/deleteNamespace',
                parameters: [string(name: 'CLUSTER', value: 'folio-testing'),
                             string(name: 'NAMESPACE', value: 'ecs-snapshot'),
                             booleanParam(name: 'RW_SPLIT', value: false),
                             string(name: 'POSTGRESQL', value: 'built-in'),
                             string(name: 'KAFKA', value: 'built-in'),
                             string(name: 'OPENSEARCH', value: 'aws'),
                             string(name: 'S3_BUCKET', value: 'aws'),
                             string(name: 'AGENT', value: 'jenkins-agent-java17'),
                             booleanParam(name: 'REFRESH_PARAMETERS', value: false)]
            } catch (Error err) {
              println("Existing env has been destroyed, error: ${err}")
            }
          } else {
            println("ECS-SNAPSHOT ENV DOES NOT EXIST...\nCONTINUING TO WORK...")
          }
        }
      }

      stage('[Provision environment...]') {
        folioTerraformFlow.manageNamespace('apply', tfConfig)
      }

      stage('[Helm Indices & DB preparation...]') {
        withCredentials([usernamePassword(credentialsId: 'elastic', passwordVariable: 'es_password', usernameVariable: 'es_username')]) {
          folioEcsIndices.prepareEcsIndices("${env.es_username}", "${env.es_password}")
        }
        folioHelm.withKubeConfig(params.CLUSTER) {
          psqlDumpMethods.restoreHelmData("psql-restore", "psql-dump", "1.0.5", "${params.SOURCE_DB_USERS}",
            "${params.SOURCE_DB}", Constants.PSQL_DUMP_BACKUPS_BUCKET_NAME,
            "ecs-snapshot", "ecs-snapshot")
        }
      }

      stage('[Helm install modules...]') {
        //tdb
      }

      stage('[New modules deploy...]') {
        //tdb
      }

      stage('[Configure edge-users & salt...]') {
        //tdb
      }

      stage('[Build ecs-snapshot UI...]') {
        //tdb
      }

    } catch (exception) {
      println(exception)
      error(exception.getMessage())
    } finally {
      stage('Cleanup') {
        cleanWs notFailBuild: true
      }
    }
  }
}
