#!groovy
package folioRancher.folioNamespaceTools.manageNamespace.createDailySnapshotECS


import org.folio.Constants
import org.folio.models.*
import org.folio.models.parameters.CreateNamespaceParameters
import org.folio.rest.GitHubUtility
import org.folio.rest_v2.Main
import org.folio.rest_v2.Users
import org.folio.utilities.Logger
import org.folio.utilities.Tools
import org.jenkinsci.plugins.workflow.libs.Library

@Library('pipelines-shared-library@RANCHER-2133') _

CONFIG_BRANCH = 'RANCHER-2133'

properties([buildDiscarder(logRotator(numToKeepStr: '30')),
            disableConcurrentBuilds(),
            parameters([booleanParam(name: 'RETRY_ENABLE', defaultValue: false, description: '(Optional) Set to true to retry install'),
                        folioParameters.refreshParameters()])
//            ,pipelineTriggers([parameterizedCron('''45 20 * * 1-5 %CLUSTER=folio-testing;NAMESPACE=ecs-snapshot;CONFIG_TYPE=testing;RETRY_ENABLE=false''')])
])

if (params.REFRESH_PARAMETERS) {
  currentBuild.result = 'ABORTED'
  return
}

ansiColor('xterm') {
  node('rancher') {

    String defaultTenantId = 'cs00000int'

    TerraformConfig tfConfig = new TerraformConfig('terraform/rancher/project')
      .withWorkspace("${params.CLUSTER}-${params.NAMESPACE}")

    RancherNamespace namespace = new RancherNamespace(params.CLUSTER, params.NAMESPACE)
      .withDefaultTenant(defaultTenantId)
      .withDeploymentConfigType(params.CONFIG_TYPE)
      .withSuperTenantAdminUser()

    namespace.addDeploymentConfig(CONFIG_BRANCH)

    Logger logger = new Logger(this, env.JOB_BASE_NAME)

    String installJsonS3Path = "${Constants.PSQL_DUMP_BACKUPS_BUCKET_NAME}/ecs-snapshot/"
    List newInstallJson = []
    InstallRequestParams installRequestParams = new InstallRequestParams(reinstall: true)
      .withTenantParameters("centralTenantId=cs00000int")
    Main main = new Main(this, namespace.generateDomain('okapi'), namespace.getSuperTenant())

    final String deleteNamespaceJobName = Constants.JENKINS_DELETE_NAMESPACE_JOB
    final String deployModulesFromJson = Constants.JENKINS_DEPLOY_MODULES_FROM_JSON

    CreateNamespaceParameters namespaceBaseParams = new CreateNamespaceParameters.Builder()
      .clusterName('folio-testing')
      .namespaceName('ecs-snapshot')
      .rwSplit(false)
      .greenmail(false)
      .mockServer(false)
      .pgType('built-in')
      .pgVersion('16.4')
      .kafkaType('built-in')
      .opensearchType('aws')
      .s3Type('built-in')
      .members('')
      .worker('rancher')
      .build()

    stage('Ini') {
      buildName "${tfConfig.getWorkspace()}-${env.BUILD_ID}"
      buildDescription "Config: ${params.CONFIG_TYPE}"
      newInstallJson = new GitHubUtility(this).getEnableList('platform-complete', 'snapshot')
      newInstallJson.removeAll { module -> module.id == 'okapi' }
    }
    stage('[TF] Set variables') {
      tfConfig.addVar('rancher_cluster_name', 'folio-testing')
      tfConfig.addVar('rancher_project_name', 'ecs-snapshot')
      tfConfig.addVar('tenant_id', defaultTenantId)
      tfConfig.addVar('pg_password', Constants.PG_ROOT_DEFAULT_PASSWORD)
      tfConfig.addVar('pgadmin_password', Constants.PGADMIN_DEFAULT_PASSWORD)
      tfConfig.addVar('pg_version', '16.4')
      tfConfig.addVar('pg_dbname', 'folio')
      tfConfig.addVar('pg_vol_size', 100)
      tfConfig.addVar('pg_embedded', true)
      tfConfig.addVar('kafka_shared', false)
      tfConfig.addVar('opensearch_shared', false)
      tfConfig.addVar('s3_embedded', true)
      tfConfig.addVar('pgadmin4', 'true')
      tfConfig.addVar('pg_ldp_user_password', Constants.PG_LDP_DEFAULT_PASSWORD)
    }
    try {
      stage('Checkout') {
        checkout scm
      }
      if (!params.RETRY_ENABLE) {
        stage('[TF] Destroy') {
          def nsExists
          folioHelm.withKubeConfig(namespaceBaseParams.getClusterName()) {
            nsExists = kubectl.checkNamespaceExistence("${namespaceBaseParams.getNamespaceName()}")
          }
          if (nsExists) {
            folioTriggerJob.deleteNamespace(deleteNamespaceJobName, namespaceBaseParams)
          } else {
            logger.warning("${namespaceBaseParams.getNamespaceName()} namespace does not exists!")
          }
        }

        stage('[TF] Provision') {
          params.RETRY_ENABLE ? null : folioTerraformFlow.manageNamespace('apply', tfConfig)
        }

        stage('[DB and Indices] Restore') {
          folioHelm.withKubeConfig(params.CLUSTER) {
            folioPrint.colored("Restoring psql ecs dump...\nEstimated duration: ~ 1-2 hours", "green")
            params.RETRY_ENABLE ? null : psqlDumpMethods.restoreHelmData("psql-restore", "psql-dump", "1.0.6", "roles", "ecs-snapshot", Constants.PSQL_DUMP_BACKUPS_BUCKET_NAME, "ecs-snapshot", "${params.NAMESPACE}")
            kubectl.patchSecret("mod-consortia-systemuser", "SYSTEM_USER_NAME", "consortia-system-user", "${params.NAMESPACE}")
            kubectl.patchSecret("mod-search-systemuser", "SYSTEM_USER_NAME", "mod-search", "${params.NAMESPACE}")
          }
        }
      }
      stage('Restore') {
        folioHelm.withK8sClient {
          namespace.getModules().setInstallJsonObject(new Tools(this)
            .jsonParse(awscli.getS3FileContent("${installJsonS3Path}" + "install.json")))
          namespace.setOkapiVersion(common.getOkapiVersion(namespace.getModules().getInstallJson()))
        }
        folioDeployFlow.restore(namespace)
        sleep time: 2, unit: "MINUTES"
      }

      stage('Prepare for update') {
        folioHelm.withK8sClient {
          def consortia_system_user_pwd = kubectl.getSecretValue(namespace.getNamespaceName(), 'mod-consortia-systemuser', 'SYSTEM_USER_PASSWORD')
          def search_system_user_pwd = kubectl.getSecretValue(namespace.getNamespaceName(), 'mod-search-systemuser', 'SYSTEM_USER_PASSWORD')
          Users users = new Users(this, namespace.generateDomain('okapi'))
          OkapiUser consortia_system_user = new OkapiUser('consortia-system-user', consortia_system_user_pwd)
          OkapiUser search_system_user = new OkapiUser('mod-search', search_system_user_pwd)

          List tenants = ['cs00000int', 'cs00000int_0001', 'cs00000int_0002', 'cs00000int_0003', 'cs00000int_0004', 'cs00000int_0005']

          tenants.each {id ->
            OkapiTenant tenant = new OkapiTenant(id)
            def user = users.getUserByName(tenant, consortia_system_user)
            consortia_system_user.setUuid(user.get('id'))
            users.resetUserPassword(tenant, consortia_system_user)

            def user2 = users.getUserByName(tenant, search_system_user)
            search_system_user.setUuid(user2.get('id'))
            users.resetUserPassword(tenant, search_system_user)
          }
        }
      }

      stage('Update') {
        Modules modules = new Modules()
        newInstallJson.add([id: 'okapi-' + folioStringScripts.getOkapiVersions()[0], action: 'enable']) // TMP workaround
        newInstallJson.add([id: modules.getModuleVersion('mod-consortia'), action: 'enable']) // TMP workaround
        newInstallJson.add([id: modules.getModuleVersion('folio_consortia-settings'), action: 'enable']) // TMP workaround
        build job: deployModulesFromJson,
          parameters: [string(name: 'CLUSTER', value: 'folio-testing'),
                       string(name: 'NAMESPACE', value: 'ecs-snapshot'),
                       string(name: 'CONFIG_TYPE', value: 'testing'),
                       string(name: 'DEFAULT_TENANT_ID', value: 'cs00000int'),
                       text(name: 'INSTALL_JSON', value: newInstallJson.toString()),
                       booleanParam(name: 'RTR', value: false),
                       booleanParam(name: 'LOAD_REFERENCE', value: false),
                       booleanParam(name: 'LOAD_SAMPLE', value: false),
                       booleanParam(name: 'SIMULATE', value: true),
                       booleanParam(name: 'REINSTALL', value: true),
                       booleanParam(name: 'IGNORE_ERRORS', value: false),
                       string(name: 'AGENT', value: 'rancher'),
                       booleanParam(name: 'REFRESH_PARAMETERS', value: false)]
      }

      stage('Build and deploy UI') {

//        slackSend(color: 'good', message: 'ecs-snapshot env successfully built\n' + "UI: https://${namespace.clusterName}-${namespace.namespaceName}-${namespace.defaultTenantId}" +
//          ".${Constants.CI_ROOT_DOMAIN}\nCredentials: ecs_admin:admin", channel: '#rancher_tests_notifications')
      }

    } catch (e) {
      stage('Notify') {
        println "Caught exception: ${e}"
//        slackSend(color: 'danger', message: "ecs-snapshot env build failed...\nConsole output: ${env.BUILD_URL}", channel: '#rancher_tests_notifications')
      }
      error(e.getMessage())
    } finally {
      stage('Cleanup') {
        cleanWs notFailBuild: true
      }
    }
  }
}
