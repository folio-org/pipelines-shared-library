#!groovy
import org.folio.Constants
import org.folio.models.OkapiTenant
import org.folio.models.RancherNamespace
import org.folio.models.TenantUi
import org.folio.models.TerraformConfig
import org.folio.rest.GitHubUtility
import org.folio.rest_v2.Main
import org.jenkinsci.plugins.workflow.libs.Library

@Library('pipelines-shared-library@RANCHER-1216') _

CONFIG_BRANCH = 'RANCHER-1216'

properties([buildDiscarder(logRotator(numToKeepStr: '20')),
            disableConcurrentBuilds(),
            parameters([folioParameters.cluster(),
                        folioParameters.namespace(),
                        folioParameters.branch(),
                        folioParameters.okapiVersion(),
                        folioParameters.configType(),
                        booleanParam(name: 'RW_SPLIT', defaultValue: false, description: '(Optional) Set true to Enable Read/Write split'),
                        string(name: 'SOURCE_DB_USERS', defaultValue: 'ecs-snapshot-users', description: '(Required) Name of users dump file'),
                        string(name: 'SOURCE_DB', defaultValue: 'ecs-snapshot', description: '(Required) Data dump file name'),
                        folioParameters.pgVersion(),
                        folioParameters.kafkaType(),
                        folioParameters.opensearchType(),
                        string(name: 'MEMBERS', defaultValue: '', description: '(Optional) Coma separated list of GitHub teams who need an access to the namespace'),
                        folioParameters.agent(),
                        folioParameters.refreshParameters()])
// TODO DISCUSS SCHEDULE AND COMPLETE PARAMETERS...
//            pipelineTriggers([parameterizedCron("55 21 * * 1-5 %CLUSTER=folio-testing;NAMESPACE=esc-snapshot;" +
//                            "blah blah blah")])
])

ansiColor('xterm') {
  if (params.REFRESH_PARAMETERS) {
    currentBuild.result = 'ABORTED'
    error('DRY RUN BUILD, PARAMETERS REFRESHED!')
  }

  node(params.AGENT) {
    try {
      stage('Ini') {
        buildName params.CLUSTER + '-' + params.NAMESPACE + '-' + env.BUILD_ID
        buildDescription "cluster: ${params.CLUSTER}\n" + "namespace: ${params.NAMESPACE}\n" + "config: ${params.CONFIG_TYPE}"
      }
      stage('Checkout') {
        checkout scm
      }

      TerraformConfig tfConfig = new TerraformConfig('terraform/rancher/project')
        .withWorkspace("${params.CLUSTER}-${params.NAMESPACE}")

      tfConfig.addVar('pg_vol_size', 100)
      tfConfig.addVar('pg_dbname', 'folio')
      tfConfig.addVar('pg_version', params.DB_VERSION)
      tfConfig.addVar('pg_password', Constants.PG_ROOT_DEFAULT_PASSWORD)
      tfConfig.addVar('pgadmin_password', Constants.PGADMIN_DEFAULT_PASSWORD)
      tfConfig.addVar('pg_embedded', true)
      tfConfig.addVar('rancher_cluster_name', params.CLUSTER)
      tfConfig.addVar('rancher_project_name', params.NAMESPACE)
      tfConfig.addVar('tenant_id', 'cs00000int')
      tfConfig.addVar('kafka_shared', params.KAFKA != 'built-in')
      tfConfig.addVar('opensearch_shared', params.OPENSEARCH != 'built-in')
      tfConfig.addVar('s3_embedded', false)
      tfConfig.addVar('pgadmin4', 'true')
      tfConfig.addVar('enable_rw_split', params.RW_SPLIT)
      tfConfig.addVar('github_team_ids', folioTools.getGitHubTeamsIds("${Constants.ENVS_MEMBERS_LIST[params.NAMESPACE]},${params.MEMBERS}").collect { "\"${it}\"" })

      String commitHash = common.getLastCommitHash('platform-complete', params.FOLIO_BRANCH)
      List installJson = new GitHubUtility(this).getEnableList('platform-complete', params.FOLIO_BRANCH)
      TenantUi tenantUi = new TenantUi("${params.CLUSTER}-${params.NAMESPACE}", commitHash, params.FOLIO_BRANCH)

      RancherNamespace namespace = new RancherNamespace(params.CLUSTER, params.NAMESPACE)
        .withSuperTenantAdminUser()
        .withOkapiVersion(params.OKAPI_VERSION)
        .withDefaultTenant('cs00000int')
        .withDeploymentConfigType(params.CONFIG_TYPE)

      List institutional = ['cs00000int_001', 'cs00000int_002', 'cs00000int_003', 'cs00000int_004', 'cs00000int_005']

      institutional.each {ecs ->
        OkapiTenant ecs_tenant = new OkapiTenant(ecs)
        namespace.addTenant(ecs_tenant)
      }

      namespace.setEnableRwSplit(params.RW_SPLIT)
      namespace.addDeploymentConfig(CONFIG_BRANCH)
      namespace.getModules().setInstallJson(installJson)

      Main main = new Main(this, namespace.getDomains()['okapi'], namespace.getSuperTenant())

      stage('[DESTROY EXISTING ENVIRONMENT...]') {
        folioHelm.withKubeConfig(params.CLUSTER) {
          if ((kubectl.checkNamespaceExistence('ecs-snapshot')) == 'ecs-snapshot') {
            folioPrint.colored("ECS-SNAPSHOT ENVIRONMENT EXISTS!\nPROCEEDING WITH DESTROY OPERATION...", "red")
            try {
              build job: '/folioRancher/folioNamespaceTools/deleteNamespace',
                parameters: [string(name: 'CLUSTER', value: 'folio-testing'),
                             string(name: 'NAMESPACE', value: 'ecs-snapshot'),
                             booleanParam(name: 'RW_SPLIT', value: false),
                             string(name: 'POSTGRESQL', value: 'built-in'),
                             string(name: 'KAFKA', value: 'built-in'),
                             string(name: 'OPENSEARCH', value: 'aws'),
                             string(name: 'S3_BUCKET', value: 'aws'),
                             string(name: 'AGENT', value: 'jenkins-agent-java17'),
                             booleanParam(name: 'REFRESH_PARAMETERS', value: false)]
            } catch (Exception exception) {
              folioPrint.colored("EXISTING ENVIRONMENT HAS NOT BEEN DESTROYED, ERROR: ${exception.getMessage()}", "red")
            }
          } else {
            folioPrint.colored("ECS-SNAPSHOT ENVIRONMENT DOES NOT EXIST...\nCONTINUING TO WORK...", "green")
          }
        }
      }

      stage('[-=PROVISION ENVIRONMENT=-]') {
        folioTerraformFlow.manageNamespace('apply', tfConfig)
      }

      stage('[-=INDICES & DB PREPARATION=-]') {
        folioPrint.colored("HANDLING ES INDICES...", "green")
        withCredentials([usernamePassword(credentialsId: 'elastic', passwordVariable: 'es_password', usernameVariable: 'es_username')]) {
          folioEcsIndices.prepareEcsIndices("${env.es_username}", "${env.es_password}")
        }
        folioHelm.withKubeConfig(params.CLUSTER) {
          folioPrint.colored("RESTORING PSQL ECS DUMP...\nESTIMATED DURATION: ~ 2 HOURS", "green")
          psqlDumpMethods.restoreHelmData("psql-restore", "psql-dump", "1.0.5", "${params.SOURCE_DB_USERS}",
            "${params.SOURCE_DB}", Constants.PSQL_DUMP_BACKUPS_BUCKET_NAME,
            "${params.SOURCE_DB}", "${params.NAMESPACE}")
        }
      }

      stage('[-=INSTALL OKAPI=-]') {
        folioHelm.withKubeConfig(namespace.getClusterName()) {
          folioHelm.deployFolioModule(namespace, 'okapi', namespace.getOkapiVersion())
          folioHelm.checkPodRunning(namespace.getNamespaceName(), 'okapi')
          folioPrint.colored("TIMEOUT FOR OKAPI INITIALIZATION PROCESS...", "green")
          sleep time: 3, unit: 'MINUTES'
        }
      }

      stage('[-=INSTALL BACKEND MODULES=-]') {
        folioPrint.colored("INSTALLING BACKEND MODULES...", "green")
        main.preInstall(namespace.getModules().getInstallJson(), namespace.getModules().getDiscoveryList())
        folioHelm.withKubeConfig(namespace.getClusterName()) {
          folioHelm.deployFolioModulesParallel(namespace, namespace.getModules().getBackendModules())
          folioHelm.checkAllPodsRunning(namespace.getNamespaceName())
        }
      }

      stage('[-=INSTALL EDGE MODULES & CONFIGURE EDGE-USERS=-]') {
        folioPrint.colored("INSTALLING EDGE MODULES...", "green")
        folioEdge.renderEphemeralProperties(namespace)
        folioHelm.withKubeConfig(namespace.getClusterName()) {
          namespace.getModules().getEdgeModules().each { name, version ->
            kubectl.createConfigMap("${name}-ephemeral-properties", namespace.getNamespaceName(), "./${name}-ephemeral-properties")
          }
          folioHelm.deployFolioModulesParallel(namespace, namespace.getModules().getEdgeModules())
        }
      }

      stage('[-=NEW MODULES DEPLOY=-]') {
        //tdb
      }

      stage('[-=BUILD ECS-SNAPSHOT UI=-]') {
        //tdb
      }

    } catch (exception) {
      println(exception)
      error(exception.getMessage())
    } finally {
      stage('CLEANUP') {
        cleanWs notFailBuild: true
      }
    }
  }
}
