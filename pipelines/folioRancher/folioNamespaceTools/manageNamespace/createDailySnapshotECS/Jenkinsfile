#!groovy
package folioRancher.folioNamespaceTools.manageNamespace.createDailySnapshotECS

import groovy.json.JsonSlurperClassic
import org.folio.Constants
import org.folio.models.*
import org.folio.models.parameters.CreateNamespaceParameters
import org.folio.rest.GitHubUtility
import org.folio.rest_v2.Main
import org.folio.rest_v2.Users
import org.folio.utilities.Logger
import org.folio.utilities.Tools
import org.jenkinsci.plugins.workflow.libs.Library

@Library('pipelines-shared-library') _

CONFIG_BRANCH = 'master'

properties([buildDiscarder(logRotator(numToKeepStr: '30')),
            disableConcurrentBuilds(),
            parameters([folioParameters.cluster(),
                        folioParameters.namespace(),
                        folioParameters.configType(),
                        booleanParam(name: 'CONSORTIA', defaultValue: false, description: '(Optional) Set to true to enable consortium'),
                        booleanParam(name: 'RW_SPLIT', defaultValue: false, description: '(Optional) Set to true to enable Read/Write split'),
                        booleanParam(name: 'RETRY_ENABLE', defaultValue: false, description: '(Optional) Set to true to retry install'),
                        folioParameters.pgType(),
                        folioParameters.kafkaType(),
                        folioParameters.opensearchType(['aws']),
                        folioParameters.s3Type(),
                        folioParameters.agent(),
                        folioParameters.refreshParameters()]),
            pipelineTriggers([parameterizedCron('''45 20 * * * %CLUSTER=folio-testing;NAMESPACE=ecs-snapshot;CONFIG_TYPE=testing;CONSORTIA=true;RW_SPLIT=false;RETRY_ENABLE=false;POSTGRESQL=built-in;KAFKA=built-in;OPENSEARCH=aws;S3_BUCKET=built-in;AGENT=rancher''')])])

if (params.REFRESH_PARAMETERS) {
  currentBuild.result = 'ABORTED'
  return
}

String defaultTenantId = 'cs00000int'
String sys_psswd = ''

TerraformConfig tfConfig = new TerraformConfig('terraform/rancher/project')
  .withWorkspace("${params.CLUSTER}-${params.NAMESPACE}")

tfConfig.addVar('rancher_cluster_name', params.CLUSTER)
tfConfig.addVar('rancher_project_name', params.NAMESPACE)
tfConfig.addVar('tenant_id', defaultTenantId)
tfConfig.addVar('pg_password', Constants.PG_ROOT_DEFAULT_PASSWORD)
tfConfig.addVar('pgadmin_password', Constants.PGADMIN_DEFAULT_PASSWORD)
tfConfig.addVar('pg_version', '16.1')
tfConfig.addVar('pg_dbname', 'folio')
tfConfig.addVar('pg_vol_size', 100)
tfConfig.addVar('pg_embedded', params.POSTGRESQL == 'built-in')
tfConfig.addVar('kafka_shared', params.KAFKA != 'built-in')
tfConfig.addVar('opensearch_shared', params.OPENSEARCH != 'built-in')
tfConfig.addVar('s3_embedded', params.S3_BUCKET == 'built-in')
tfConfig.addVar('pgadmin4', 'true')
tfConfig.addVar('pg_ldp_user_password', Constants.PG_LDP_DEFAULT_PASSWORD)
tfConfig.addVar('github_team_ids', folioTools.getGitHubTeamsIds("${Constants.ENVS_MEMBERS_LIST[params.NAMESPACE]},${params.MEMBERS}").collect { "\"${it}\"" })

RancherNamespace namespace = new RancherNamespace(params.CLUSTER, params.NAMESPACE)
  .withDefaultTenant(defaultTenantId)
  .withDeploymentConfigType(params.CONFIG_TYPE)
  .withSuperTenantAdminUser()

namespace.addDeploymentConfig(CONFIG_BRANCH)

Logger logger = new Logger(this, env.JOB_BASE_NAME)

String installJsonS3Path = "${Constants.PSQL_DUMP_BACKUPS_BUCKET_NAME}/ecs-snapshot/"
List newInstallJson = new GitHubUtility(this).getEnableList('platform-complete', 'snapshot')
InstallRequestParams installRequestParams = new InstallRequestParams(reinstall: true)
  .withTenantParameters("centralTenantId=cs00000int")
Main main = new Main(this, namespace.generateDomain('okapi'), namespace.getSuperTenant())

final String deleteNamespaceJobName = Constants.JENKINS_DELETE_NAMESPACE_JOB

CreateNamespaceParameters namespaceBaseParams = new CreateNamespaceParameters.Builder()
  .clusterName('folio-testing')
  .namespaceName('ecs-snapshot')
  .rwSplit(false)
  .greenmail(false)
  .mockServer(false)
  .pgType('built-in')
  .pgVersion('12.15')
  .kafkaType('built-in')
  .opensearchType('aws')
  .s3Type('built-in')
  .members('')
  .worker('rancher')
  .build()

ansiColor('xterm') {
  node(params.AGENT) {
    stage('Ini') {
      buildName "${tfConfig.getWorkspace()}-${env.BUILD_ID}"
      buildDescription "Config: ${params.CONFIG_TYPE}"
    }
    try {
      stage('Checkout') {
        checkout scm
      }
      if (!params.RETRY_ENABLE) {
        stage('[TF] Destroy') {
          def nsExists
          folioHelm.withKubeConfig(namespaceBaseParams.getClusterName()) {
            nsExists = kubectl.checkNamespaceExistence("${namespaceBaseParams.getNamespaceName()}")
          }
          if (nsExists) {
            folioTriggerJob.deleteNamespace(deleteNamespaceJobName, namespaceBaseParams)
          } else {
            logger.warning("${namespaceBaseParams.getNamespaceName()} namespace does not exists!")
          }
        }

        stage('[TF] Provision') {
          folioTerraformFlow.manageNamespace('apply', tfConfig)
        }

        stage('[DB and Indices] Restore') {
          folioPrint.colored("Restoring indices...", "green")
          withCredentials([usernamePassword(credentialsId: 'elastic', passwordVariable: 'es_password', usernameVariable: 'es_username')]) {
            folioEcsIndices.prepareEcsIndices("${env.es_username}", "${env.es_password}")
          }

          folioHelm.withKubeConfig(params.CLUSTER) {
            folioPrint.colored("Restoring psql ecs dump...\nEstimated duration: ~ 1-2 hours", "green")
            psqlDumpMethods.restoreHelmData("psql-restore", "psql-dump", "1.0.6", "ecs-snapshot-users",
              "ecs-snapshot-01", Constants.PSQL_DUMP_BACKUPS_BUCKET_NAME,
              "ecs-snapshot", "${params.NAMESPACE}")
            sys_psswd = kubectl.getSecretValue(namespace.getNamespaceName(), 'mod-consortia-systemuser', 'SYSTEM_USER_PASSWORD')
            kubectl.patchSecret("mod-consortia-systemuser", "SYSTEM_USER_NAME", "consortia-system-user", "ecs-snapshot")
          }
        }
      }
      stage('Restore') {
        folioHelm.withK8sClient {
          namespace.getModules().setInstallJsonObject(new Tools(this)
            .jsonParse(awscli.getS3FileContent("${installJsonS3Path}" + "install.json")))
          namespace.setOkapiVersion(common.getOkapiVersion(namespace.getModules().getInstallJson()))
        }
        folioDeployFlow.restore(namespace)
        sleep time: 5, unit: "MINUTES"
      }

      stage('Prepare update') {
        main.getTenantsList().each {
          switch (it) {
            case ['cs00000int', 'cs00000int_0001', 'cs00000int_0002', 'cs00000int_0003', 'cs00000int_0004', 'cs00000int_0005']:
              namespace.addTenant(new OkapiTenant("${it}")
                .withAdminUser(new OkapiUser('ecs_admin', 'admin'))
                .withInstallRequestParams(installRequestParams)
                .withConfiguration(new OkapiConfig()))
              break
          }
        }
        Users users = new Users(this, namespace.generateDomain('okapi'))
        namespace.getModules().setInstallJsonObject(newInstallJson)
        OkapiUser okapiUser = new OkapiUser('consortia-system-user', sys_psswd)
        namespace.setEnableConsortia(params.CONSORTIA, false)
        namespace.tenants.each { name, id ->
          if (!params.RETRY_ENABLE) {
            def ldp_disable = """[{"id": "mod-ldp-1.1.3","action": "disable"},{"id": "folio_ldp-2.1.0","action": "disable"}]"""
            main.tenantInstall(id, new JsonSlurperClassic().parseText(ldp_disable) as List)
          }
          switch (id.getTenantId()) {
            case ['cs00000int']:
              id.getModules().setInstallJson(newInstallJson)
              break
            case ['cs00000int_0001', 'cs00000int_0002', 'cs00000int_0003', 'cs00000int_0004', 'cs00000int_0005']:
              id.getModules().setInstallJson(newInstallJson)
              id.getModules().removeModule('folio_consortia-settings')
              break
            default:
              println("${id.tenantId} --> leaving blank")
              break
          }
          def user = users.getUserByName(id, okapiUser)
          okapiUser.setUuid(user.get('id'))
          users.resetUserPassword(id, okapiUser)
          println('TenantId: ' + id.getTenantId() + "\nModules to enable:\n" + id.getModules().getInstallJson())
        }
        main.publishServiceDiscovery(namespace.getModules().getDiscoveryList())
        main.publishDescriptors(namespace.getModules().getInstallJson())
      }

      stage('Update') {
        folioHelm.withKubeConfig(namespace.getClusterName()) {
          folioHelm.deployFolioModulesParallel(namespace, namespace.getModules().getBackendModules())

          folioEdge.renderEphemeralProperties(namespace)
          namespace.getModules().getEdgeModules().each { module ->
            kubectl.deleteConfigMap("${module.name}-ephemeral-properties", namespace.getNamespaceName())
            kubectl.createConfigMap("${module.name}-ephemeral-properties", namespace.getNamespaceName(), "./${name}-ephemeral-properties")
          }

          folioHelm.deployFolioModulesParallel(namespace, namespace.getModules().getEdgeModules())
          folioHelm.checkAllPodsRunning(namespace.getNamespaceName())
        }
      }

      stage('Enable') {
        retry(2) {
          sleep time: 5, unit: 'MINUTES' //mod-agreements, service-interaction etc | federation lock
          main.update(namespace.getTenants())
          main.runIndex(namespace.tenants[defaultTenantId], new Index('location', true, false))
        }
      }

      stage('Build and deploy UI') {
        namespace.getModules().setInstallJsonObject(newInstallJson)
        String folioBranch = 'snapshot'
        String commitHash = common.getLastCommitHash("platform-complete", "snapshot")
        namespace.setEnableConsortia(true)

        TenantUi tenantUi = new TenantUi("${params.CLUSTER}-${params.NAMESPACE}", commitHash, folioBranch)

        namespace.addTenant(new OkapiTenant(defaultTenantId)
          .withInstallRequestParams(installRequestParams)
          .withConfiguration(new OkapiConfig())
          .withTenantUi(tenantUi.clone()))

        namespace.getTenants().each { tenantId, tenant ->
          if (tenant.getTenantUi()) {
            TenantUi ui = tenant.getTenantUi()
            def jobParameters = [tenant_id  : ui.getTenantId(),
                                 custom_hash: ui.getHash(),
                                 custom_url : "https://${namespace.getDomains()['okapi']}",
                                 custom_tag : ui.getTag(),
                                 consortia  : true]
            uiBuild(jobParameters)
            folioHelm.withKubeConfig(namespace.getClusterName()) {
              folioHelm.deployFolioModule(namespace, 'ui-bundle', ui.getTag(), false, ui.getTenantId())
            }
          }
        }

        slackSend(color: 'good', message: 'ecs-snapshot env successfully built', channel: '#rancher_tests_notifications')
      }
      stage('The rest components placeholder') {
        // GreenMail, LDP, Mock server, etc.
      }

    } catch (e) {
      stage('Notify') {
        println "Caught exception: ${e}"
        slackSend(color: 'danger', message: "ecs-snapshot env build failed...", channel: '#rancher_tests_notifications')
      }
      error(e.getMessage())
    } finally {
      stage('Cleanup') {
        cleanWs notFailBuild: true
      }
    }
  }
}
