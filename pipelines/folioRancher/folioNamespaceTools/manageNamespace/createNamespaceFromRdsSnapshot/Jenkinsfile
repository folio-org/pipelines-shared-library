#!groovy
import org.folio.Constants
import org.folio.models.InstallRequestParams
import org.folio.models.OkapiTenantConsortia
import org.folio.models.RancherNamespace
import org.folio.models.TenantUi
import org.folio.models.TerraformConfig
import org.folio.rest_v2.Edge
import org.folio.rest_v2.Main
import org.folio.utilities.Tools
import org.jenkinsci.plugins.workflow.libs.Library

@Library('pipelines-shared-library@RANCHER-741-Jenkins-Enhancements') _

CONFIG_BRANCH = 'RANCHER-741-Jenkins-Enhancements'

properties([
  buildDiscarder(logRotator(numToKeepStr: '30')),
  disableConcurrentBuilds(),
  parameters([
    folioParameters.cluster(),
    folioParameters.namespace(),
    folioParameters.configType(),
    folioParameters.loadReference(false),
    folioParameters.loadSample(false),
    string(name: 'RDS_SNAPSHOT_NAME', defaultValue: '', description: 'RDS snapshot name'),
    booleanParam(name: 'RW_SPLIT', defaultValue: true, description: '(Optional) Set true to Enable Read/Write split'),
    folioParameters.pgType(['aws']),
    folioParameters.kafkaType(Constants.AWS_INTEGRATED_SERVICE_TYPE.reverse()),
    folioParameters.opensearchType(),
    folioParameters.s3Type(),
    folioParameters.agent(),
    folioParameters.refreshParameters()
  ])
])

String defaultTenantId = 'fs09000000'
String snapshotS3Path = "${Constants.PSQL_DUMP_BACKUPS_BUCKET_NAME}/rds/${params.RDS_SNAPSHOT_NAME}"

//Set terraform configuration
TerraformConfig tfConfig = new TerraformConfig('terraform/rancher/project')
  .withWorkspace("${params.CLUSTER}-${params.NAMESPACE}")

tfConfig.addVar('rancher_cluster_name', params.CLUSTER)
tfConfig.addVar('rancher_project_name', params.NAMESPACE)
tfConfig.addVar('tenant_id', defaultTenantId)
tfConfig.addVar('pg_password', Constants.PG_ROOT_DEFAULT_PASSWORD)
tfConfig.addVar('pgadmin_password', Constants.PGADMIN_DEFAULT_PASSWORD)
tfConfig.addVar('pg_embedded', params.POSTGRESQL == 'built-in')
tfConfig.addVar('kafka_shared', params.KAFKA != 'built-in')
tfConfig.addVar('opensearch_shared', params.OPENSEARCH != 'built-in')
tfConfig.addVar('s3_embedded', params.S3_BUCKET == 'built-in')
tfConfig.addVar('pgadmin4', 'true')
tfConfig.addVar('enable_rw_split', params.RW_SPLIT)
tfConfig.addVar('pg_ldp_user_password', Constants.PG_LDP_DEFAULT_PASSWORD)
tfConfig.addVar('github_team_ids', folioTools.getGitHubTeamsIds("${Constants.ENVS_MEMBERS_LIST[params.NAMESPACE]},${params.MEMBERS}").collect { "\"${it}\"" })

InstallRequestParams installRequestParams = new InstallRequestParams()
  .withTenantParameters("loadReference=${params.LOAD_REFERENCE},loadSample=${params.LOAD_SAMPLE}")

RancherNamespace namespace = new RancherNamespace(params.CLUSTER, params.NAMESPACE)
  .withSuperTenantAdminUser()
  .withDefaultTenant(defaultTenantId)
  .withDeploymentConfigType(params.CONFIG_TYPE)

namespace.addDeploymentConfig(CONFIG_BRANCH)
namespace.setEnableRwSplit(params.RW_SPLIT)

Main main = new Main(this, namespace.getDomains()['okapi'], namespace.getSuperTenant())
Edge edge = new Edge(this, namespace.getDomains()['okapi'])

ansiColor('xterm') {
  if (params.REFRESH_PARAMETERS) {
    currentBuild.result = 'ABORTED'
    error('DRY RUN BUILD, PARAMETERS REFRESHED!')
  }
  node(params.AGENT) {
    stage('Ini') {
      buildName "${tfConfig.getWorkspace()}.${env.BUILD_ID}"
      buildDescription "Snapshot: ${params.RDS_SNAPSHOT_NAME}"
    }
    try {
      stage('Checkout') {
        checkout scm
      }

      stage('Set properties') {
        String commitHash = ''
        String folioBranch = ''

        folioHelm.withK8sClient {
          namespace.getModules().setInstallJson(new Tools(this)
            .jsonParse(awscli.getS3FileContent("${snapshotS3Path}/${params.RDS_SNAPSHOT_NAME}-install.json")))
          Map uiJson = new Tools(this)
            .jsonParse(awscli.getS3FileContent("${snapshotS3Path}/${params.RDS_SNAPSHOT_NAME}-ui.json"))
          folioBranch = uiJson['branch']
          commitHash = uiJson['hash']
        }

        TenantUi tenantUi = new TenantUi("${params.CLUSTER}-${params.NAMESPACE}", commitHash, folioBranch)

        namespace.setOkapiVersion(common.getOkapiVersion(namespace.getModules().getInstallJson()))

        namespace.addTenant(folioDefault.tenants()[namespace.getDefaultTenantId()]
          .withInstallJson(namespace.getModules().getInstallJson().collect())
          .withInstallRequestParams(installRequestParams.clone())
          .withTenantUi(tenantUi.clone())
        )
      }

      stage('[Terraform] Provision') {

        tfConfig.addVar('pg_rds_snapshot_name', params.RDS_SNAPSHOT_NAME)
        tfConfig.addVar('pg_dbname', Constants.BUGFEST_SNAPSHOT_DBNAME)

        folioHelm.withK8sClient {
          tfConfig.addVar('pg_version',
            awscli.getRdsClusterSnapshotEngineVersion("us-west-2", params.RDS_SNAPSHOT_NAME))
          tfConfig.addVar('pg_username',
            awscli.getRdsClusterSnapshotMasterUsername("us-west-2", params.RDS_SNAPSHOT_NAME))
        }

        folioTerraformFlow.manageNamespace('apply', tfConfig)
      }

      stage('[Helm] Deploy Okapi') {
        folioHelm.withKubeConfig(namespace.getClusterName()) {
          folioHelm.deployFolioModule(namespace, 'okapi', namespace.getOkapiVersion())
          folioHelm.checkPodRunning(namespace.getNamespaceName(), 'okapi')
        }
      }

      stage('[Rest] Okapi healthcheck') {
        sleep time: 3, unit: 'MINUTES'
        common.healthCheck("https://${namespace.getDomains()['okapi']}/_/proxy/health")
      }

      stage('[Rest] Preinstall') {
        main.cleanServicesDiscovery()
        main.preInstall(namespace.getModules().getInstallJson(), namespace.getModules().getDiscoveryList())
      }

      stage('[Helm] Deploy backend') {
        folioHelm.withKubeConfig(namespace.getClusterName()) {
          folioHelm.deployFolioModulesParallel(namespace, namespace.getModules().getBackendModules())
          folioHelm.checkAllPodsRunning(namespace.getNamespaceName())
        }
      }

      stage('[Rest] Update') {
        sleep time: 3, unit: 'MINUTES'
        main.update(namespace.getTenants())
      }

      stage('[Rest] Configure edge') {
        edge.renderEphemeralProperties(namespace.getTenants()[namespace.getDefaultTenantId()])
        edge.createEdgeUsers(namespace.getTenants()[namespace.getDefaultTenantId()])
      }

      stage('[Helm] Deploy edge') {
        folioHelm.withKubeConfig(namespace.getClusterName()) {
          namespace.getModules().getEdgeModules().each { name, version ->
            kubectl.createConfigMap("${name}-ephemeral-properties", namespace.getNamespaceName(), "./${name}-ephemeral-properties")
          }
          folioHelm.deployFolioModulesParallel(namespace, namespace.getModules().getEdgeModules())
        }
      }

      stage('Build and deploy UI') {
        Map branches = [:]
        namespace.getTenants().each { tenantId, tenant ->
          if (tenant.getTenantUi()) {
            TenantUi ui = tenant.getTenantUi()
            branches[tenantId] = {
              def jobParameters = [
                tenant_id  : ui.getTenantId(),
                custom_hash: ui.getHash(),
                custom_url : "https://${namespace.getDomains()['okapi']}",
                custom_tag : ui.getTag(),
                consortia  : tenant instanceof OkapiTenantConsortia
              ]
              uiBuild(jobParameters)
              folioHelm.withKubeConfig(namespace.getClusterName()) {
                folioHelm.deployFolioModule(namespace, 'ui-bundle', ui.getTag(), false, ui.getTenantId())
              }
            }
          }
        }
        parallel branches
      }

      stage('Deploy ldp') {
        println('LDP deployment')
      }
    } catch (e) {
      println "Caught exception: ${e}"
      error(e.getMessage())
    } finally {
      stage('Cleanup') {
        cleanWs notFailBuild: true
      }
    }
  }
}
